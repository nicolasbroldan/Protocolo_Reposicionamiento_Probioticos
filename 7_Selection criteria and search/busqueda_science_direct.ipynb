{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pybliometrics\n",
      "  Downloading pybliometrics-3.5.2-py2.py3-none-any.whl (74 kB)\n",
      "     ---------------------------------------- 0.0/74.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 74.2/74.2 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\3060\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pybliometrics) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\3060\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pybliometrics) (4.66.1)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\3060\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pybliometrics) (2.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\3060\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->pybliometrics) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\3060\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->pybliometrics) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\3060\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->pybliometrics) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\3060\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->pybliometrics) (0.4.6)\n",
      "Installing collected packages: pybliometrics\n",
      "Successfully installed pybliometrics-3.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pybliometrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Definir la lista de probióticos y líneas celulares\n",
    "probiotics = ['Lactobacillus plantarum', 'Lactobacillus paracasei', 'Lactobacillus acidophilus', 'Lactobacillus casei', \n",
    "              'Lactobacillus rhamnosus', 'Lactobacillus crispatus', 'Lactobacillus gasseri', 'Lactobacillus reuteri', \n",
    "              'Lactobacillus bulgaricus', 'Propionibacterium jensenii', 'Propionibacterium freudenreichii', \n",
    "              'Peptostreptococcus productus', 'Bacillus coagulans', 'Bacillus subtilis', 'Bacillus laterosporus', \n",
    "              'Lactococcus lactis', 'Lactococcus reuteri', 'Lactococcus rhamnosus', 'Lactococcus casei', \n",
    "              'Lactococcus acidophilus', 'Lactococcus curvatus', 'Lactococcus plantarum', 'Enterococcus faecium', \n",
    "              'Pediococcus acidilactici', 'Pediococcus pentosaceus', 'Streptococcus sanguis', 'Streptococcus oralis', \n",
    "              'Streptococcus mitis', 'Streptococcus thermophilus', 'Streptococcus salivarius', 'Bifidobacterium longum', \n",
    "              'Bifidobacterium catenulatum', 'Bifidobacterium breve', 'Bifidobacterium animalis', 'Bifidobacterium bifidum', \n",
    "              'Bacteroides uniformis', 'Akkermansia muciniphila', 'Saccharomyces boulardii']\n",
    "\n",
    "cell_lines = ['Caco-2', 'HT-29']\n",
    "\n",
    "# Construir la cadena de búsqueda\n",
    "search_string = '\"cancer\" AND \"colon\" AND (\"RNA-seq\" OR \"transcriptome\")'\n",
    "\n",
    "# Inicializar lista para almacenar información de artículos\n",
    "all_articles_info_scopus = []\n",
    "\n",
    "# Iterar sobre probióticos y líneas celulares\n",
    "for probiotic in probiotics:\n",
    "    for cell_line in cell_lines:\n",
    "        # Construir consulta específica para cada combinación de probiótico y línea celular\n",
    "        query = f'\"{probiotic}\" AND \"{cell_line}\" AND {search_string}'\n",
    "        \n",
    "        try:\n",
    "            # Configurar la URL de la API de Scopus\n",
    "            scopus_url = 'https://api.elsevier.com/content/search/scopus'\n",
    "            \n",
    "            # Configurar los parámetros de la solicitud\n",
    "            params = {\n",
    "                'query': query,\n",
    "                'apiKey': 'bc30d438869a09fe0248ca59c3cb0f53'\n",
    "            }\n",
    "            \n",
    "            # Realizar solicitud a la API de Scopus\n",
    "            response = requests.get(scopus_url, params=params)\n",
    "            response.raise_for_status()  # Lanzar una excepción en caso de error HTTP\n",
    "            \n",
    "            # Obtener datos de la respuesta JSON\n",
    "            data = response.json()\n",
    "            \n",
    "            # Recopilar información de los artículos encontrados\n",
    "            for entry in data.get('search-results', {}).get('entry', []):\n",
    "                article_info = {\n",
    "                    'Probiotic': probiotic,\n",
    "                    'Cell_Line': cell_line,\n",
    "                    'Title': entry.get('dc:title', ''),\n",
    "                    'Authors': ', '.join(entry.get('dc:creator', [])),\n",
    "                    'Journal': entry.get('prism:publicationName', ''),\n",
    "                    'Year': entry.get('prism:coverDisplayDate', '').split('-')[0],\n",
    "                    'DOI': entry.get('prism:doi', ''),\n",
    "                    'Scopus_ID': entry.get('dc:identifier', ''),\n",
    "                    'URL': f'https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp={entry.get(\"dc:identifier\", \"\").replace(\"SCOPUS_ID:\", \"\")}&origin=inward',\n",
    "                    'cited_by':entry.get('citedby-count', ''),\n",
    "                }\n",
    "                all_articles_info_scopus.append(article_info)\n",
    "            \n",
    "            # Agregar un retraso de 1 segundo entre las solicitudes\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en la búsqueda para {probiotic} y {cell_line}: {e}\")\n",
    "\n",
    "\n",
    "# Crear DataFrame de Pandas y guardar en Excel\n",
    "df_scopus = pd.DataFrame(all_articles_info_scopus)\n",
    "df_scopus.to_excel('science_direct_articles_cancer_colon.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar un archivo Excel en un DataFrame\n",
    "file_path = 'C:\\\\Users\\\\3060\\\\OneDrive\\\\Documentos\\\\Tesis de Grado\\\\science_direct_articles_cancer_colon.xlsx'  # Reemplaza esto con la ruta de tu archivo Excel\n",
    "df_clean = pd.read_excel(file_path)\n",
    "\n",
    "# Eliminar filas duplicadas basadas en el título\n",
    "df_clean = df_clean.drop_duplicates(subset='Title')\n",
    "# Eliminar las filas con título vacío\n",
    "df_clean = df_clean.dropna(subset=['Title'])\n",
    "\n",
    "# Ordenar DataFrame por la columna 'cited_by' de mayor a menor\n",
    "#df_clean = df_clean.sort_values(by='cited_by', ascending=False)\n",
    "\n",
    "# Guardar el DataFrame actualizado en un nuevo archivo Excel\n",
    "df_clean.to_excel('science_direct_articles_cleaned_cancer_colon.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar un archivo Excel en un DataFrame\n",
    "file_path = 'C:\\\\Users\\\\3060\\\\OneDrive\\\\Documentos\\\\Tesis de Grado\\\\science_direct_articles.xlsx'  # Reemplaza esto con la ruta de tu archivo Excel\n",
    "df_clean = pd.read_excel(file_path)\n",
    "\n",
    "# Eliminar filas duplicadas basadas en el título\n",
    "df_clean = df_clean.drop_duplicates(subset='Title')\n",
    "# Eliminar las filas con título vacío\n",
    "df_clean = df_clean.dropna(subset=['Title'])\n",
    "\n",
    "# Ordenar DataFrame por la columna 'cited_by' de mayor a menor\n",
    "df_clean = df_clean.sort_values(by='cited_by', ascending=False)\n",
    "\n",
    "# Guardar el DataFrame actualizado en un nuevo archivo Excel\n",
    "df_clean.to_excel('science_direct_articles_cleaned.xlsx', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Definir la lista de probióticos y líneas celulares\n",
    "probiotics = ['Lactobacillus plantarum', 'Lactobacillus paracasei', 'Lactobacillus acidophilus', 'Lactobacillus casei', \n",
    "              'Lactobacillus rhamnosus', 'Lactobacillus crispatus', 'Lactobacillus gasseri', 'Lactobacillus reuteri', \n",
    "              'Lactobacillus bulgaricus', 'Propionibacterium jensenii', 'Propionibacterium freudenreichii', \n",
    "              'Peptostreptococcus productus', 'Bacillus coagulans', 'Bacillus subtilis', 'Bacillus laterosporus', \n",
    "              'Lactococcus lactis', 'Lactococcus reuteri', 'Lactococcus rhamnosus', 'Lactococcus casei', \n",
    "              'Lactococcus acidophilus', 'Lactococcus curvatus', 'Lactococcus plantarum', 'Enterococcus faecium', \n",
    "              'Pediococcus acidilactici', 'Pediococcus pentosaceus', 'Streptococcus sanguis', 'Streptococcus oralis', \n",
    "              'Streptococcus mitis', 'Streptococcus thermophilus', 'Streptococcus salivarius', 'Bifidobacterium longum', \n",
    "              'Bifidobacterium catenulatum', 'Bifidobacterium breve', 'Bifidobacterium animalis', 'Bifidobacterium bifidum', \n",
    "              'Bacteroides uniformis', 'Akkermansia muciniphila', 'Saccharomyces boulardii']\n",
    "\n",
    "cell_lines = ['Caco-2', 'HT-29']\n",
    "\n",
    "# Construir la cadena de búsqueda\n",
    "search_string = '\"cancer\" AND \"colon\" AND (\"Metabolomics\" OR \"Proteomics\")'\n",
    "\n",
    "# Inicializar lista para almacenar información de artículos\n",
    "all_articles_info_scopus = []\n",
    "\n",
    "# Iterar sobre probióticos y líneas celulares\n",
    "for probiotic in probiotics:\n",
    "    for cell_line in cell_lines:\n",
    "        # Construir consulta específica para cada combinación de probiótico y línea celular\n",
    "        query = f'\"{probiotic}\" AND \"{cell_line}\" AND {search_string}'\n",
    "        \n",
    "        try:\n",
    "            # Configurar la URL de la API de Scopus\n",
    "            scopus_url = 'https://api.elsevier.com/content/search/scopus'\n",
    "            \n",
    "            # Configurar los parámetros de la solicitud\n",
    "            params = {\n",
    "                'query': query,\n",
    "                'apiKey': 'bc30d438869a09fe0248ca59c3cb0f53'\n",
    "            }\n",
    "            \n",
    "            # Realizar solicitud a la API de Scopus\n",
    "            response = requests.get(scopus_url, params=params)\n",
    "            response.raise_for_status()  # Lanzar una excepción en caso de error HTTP\n",
    "            \n",
    "            # Obtener datos de la respuesta JSON\n",
    "            data = response.json()\n",
    "            \n",
    "            # Recopilar información de los artículos encontrados\n",
    "            for entry in data.get('search-results', {}).get('entry', []):\n",
    "                article_info = {\n",
    "                    'Probiotic': probiotic,\n",
    "                    'Cell_Line': cell_line,\n",
    "                    'Title': entry.get('dc:title', ''),\n",
    "                    'Authors': ', '.join(entry.get('dc:creator', [])),\n",
    "                    'Journal': entry.get('prism:publicationName', ''),\n",
    "                    'Year': entry.get('prism:coverDisplayDate', '').split('-')[0],\n",
    "                    'DOI': entry.get('prism:doi', ''),\n",
    "                    'Scopus_ID': entry.get('dc:identifier', ''),\n",
    "                    'URL': f'https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp={entry.get(\"dc:identifier\", \"\").replace(\"SCOPUS_ID:\", \"\")}&origin=inward',\n",
    "                    'cited_by':entry.get('citedby-count', ''),\n",
    "                }\n",
    "                all_articles_info_scopus.append(article_info)\n",
    "            \n",
    "            # Agregar un retraso de 1 segundo entre las solicitudes\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en la búsqueda para {probiotic} y {cell_line}: {e}\")\n",
    "\n",
    "\n",
    "# Crear DataFrame de Pandas y guardar en Excel\n",
    "df_scopus = pd.DataFrame(all_articles_info_scopus)\n",
    "df_scopus.to_excel('science_direct_articles_cancer_colon.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
